{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Clean up\r\n",
        "# spark.sql(\"DROP DATABASE IF EXISTS demoDataSparkDb CASCADE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "microsoft": {},
        "collapsed": false
      },
      "source": [
        "# Load the data from storage\r\n",
        "dataSource = 'abfss://demo@stdemodatalake001.dfs.core.windows.net/'\r\n",
        "path = 'nycTripYellow2019Jan/nycTripYellow2019Jan.snappy.parquet'\r\n",
        "fileFormat = 'parquet'\r\n",
        "\r\n",
        "df = spark.read.load(dataSource + path, format=fileFormat)\r\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Show the column name & data type\r\n",
        "display(df.dtypes)\r\n",
        "\r\n",
        "# Column data type issue: vendorID, puLocationId, doLocationId, paymentType, improvementSurcharge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Build some baseline understanding of the data. (Max, min, etc)\r\n",
        "\r\n",
        "display(df.describe())\r\n",
        "\r\n",
        "# Same issue as in SQL: \r\n",
        "#  > PassengerCount: 0 - 9\r\n",
        "#  > FareAmount: negative - 623k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check for pickup time >= DropOff time\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "dfTimeCheck = df.filter(col('tpepPickupDateTime') >= col('tpepDropoffDateTime')).count()\r\n",
        "display(dfTimeCheck)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# For the fareAmount, we want to conduct a more detail analysis\r\n",
        "# We want to exclude the fares on the extreme values using Normal distribution (99.7% coverage).\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "dfFareAmountPercentile = df.filter(col(\"fareAmount\") > 0).select(percentile_approx(df[\"fareAmount\"], [0.0015, 0.9985]).alias(\"percentRank\"))\r\n",
        "\r\n",
        "display(dfFareAmountPercentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a dataframe based on the data quality issues we found so far.\r\n",
        "\r\n",
        "dfClean = df.filter(\r\n",
        "    (col(\"passengerCount\") > 0) &\r\n",
        "    (col(\"tripDistance\") > 0) &\r\n",
        "    (col('tpepDropoffDateTime') > col('tpepPickupDateTime')) &\r\n",
        "    (col(\"fareAmount\") >= 2.5) &\r\n",
        "    (col(\"fareAmount\") <= 75)\r\n",
        ")\r\n",
        "\r\n",
        "display(dfClean.count())\r\n",
        "\r\n",
        "# 7664528 -> 7477805\r\n",
        "# ~186k records have issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Lets create the clean table\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS demoDataSparkDb\")\r\n",
        "\r\n",
        "dfClean = dfClean.drop(\"startLon\", \"startLat\", \"endLon\", \"endLat\")\r\n",
        "dfClean.withColumn(\"vendorId\", col(\"vendorId\").cast(IntegerType()))\r\n",
        "dfClean.withColumn(\"puLocationId\", col(\"puLocationId\").cast(IntegerType()))\r\n",
        "dfClean.withColumn(\"doLocationId\", col(\"doLocationId\").cast(IntegerType()))\r\n",
        "dfClean.withColumn(\"paymentType\", col(\"paymentType\").cast(IntegerType()))\r\n",
        "dfClean.withColumn(\"improvementSurcharge\", col(\"improvementSurcharge\").cast(DoubleType()))\r\n",
        "\r\n",
        "dfClean.write.mode(\"overwrite\").saveAsTable(\"demoDataSparkDb.clean_nycTripYellow2019Jan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dfTest = spark.read.table(\"demoDataSparkDb.clean_nycTripYellow2019Jan\")\r\n",
        "\r\n",
        "display(dfTest.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Lets plot this data to see the data as we expect a relationship between trip distance and total amount.\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from pandas.plotting import scatter_matrix\r\n",
        "\r\n",
        "# Clear the plot\r\n",
        "plt.clf()\r\n",
        "\r\n",
        "# We can add additional variables in the select for additional analysis.\r\n",
        "#  does the fareAmount has any relationship wtih the tripDistance? \r\n",
        "pdComplexCheck = dfTest.select(\"tripDistance\", \"fareAmount\").toPandas()\r\n",
        "pd.plotting.scatter_matrix(pdComplexCheck, figsize=(12,12))\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# As we can see, there are some outlier values for trip distance as well. \r\n",
        "# The same approach can be applied. \r\n",
        "\r\n",
        "# This is not to say, for trips that are < $2.5 and > $75 are not valid records. \r\n",
        "# We should discuss with the client to ensure the data is accurate. \r\n",
        "\r\n",
        "# Observation (Right)\r\n",
        "# - Majority of the fare is < $15\r\n",
        "# - We have some outliers where the fare is $10 but its going 700 miles"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}
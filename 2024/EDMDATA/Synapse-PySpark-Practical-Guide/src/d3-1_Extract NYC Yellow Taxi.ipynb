{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Parameters for my storage\r\n",
        "\r\n",
        "accountName = \"stdemodatalake001\"\r\n",
        "containerName = \"demo\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure storage access info\r\n",
        "blob_account_name = \"azureopendatastorage\"\r\n",
        "blob_container_name = \"nyctlc\"\r\n",
        "blob_relative_path = \"yellow\"\r\n",
        "blob_sas_token = r\"\"\r\n",
        "\r\n",
        "# Allow SPARK to read from Blob remotely\r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "spark.conf.set(\r\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n",
        "  blob_sas_token)\r\n",
        "print('Remote blob path: ' + wasbs_path)\r\n",
        "\r\n",
        "# SPARK read parquet, note that it won't load any data yet by now\r\n",
        "srcDf = spark.read.parquet(wasbs_path)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demoDA",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "2",
              "normalized_state": "finished",
              "queued_time": "2024-10-13T23:38:02.9574026Z",
              "session_start_time": "2024-10-13T23:38:03.0381861Z",
              "execution_start_time": "2024-10-13T23:39:00.5908456Z",
              "execution_finish_time": "2024-10-13T23:39:09.5835228Z",
              "parent_msg_id": "858a4995-705f-4670-9650-c2c940e8bd04"
            },
            "text/plain": "StatementMeta(demoDA, 2, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "dataSource = f\"abfss://{containerName}@{accountName}.dfs.core.windows.net/\"\r\n",
        "\r\n",
        "df2016 = srcDf.filter(col('puYear') == 2016)\r\n",
        "df2017 = srcDf.filter(col('puYear') == 2017)\r\n",
        "df2018 = srcDf.filter(col('puYear') == 2018)\r\n",
        "\r\n",
        "df2016.write.mode(\"overwrite\").option(\"compression\",\"snappy\").parquet(dataSource + \"nycTripYellow2016/\")\r\n",
        "df2017.write.mode(\"overwrite\").option(\"compression\",\"snappy\").parquet(dataSource + \"nycTripYellow2017/\")\r\n",
        "df2018.write.mode(\"overwrite\").option(\"compression\",\"snappy\").parquet(dataSource + \"nycTripYellow2018/\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demoDA",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "2",
              "normalized_state": "finished",
              "queued_time": "2024-10-13T23:46:27.6809361Z",
              "session_start_time": null,
              "execution_start_time": "2024-10-13T23:46:27.8605939Z",
              "execution_finish_time": "2024-10-13T23:50:54.9232621Z",
              "parent_msg_id": "ccc140e1-f45d-4297-adce-ffabb93a90e6"
            },
            "text/plain": "StatementMeta(demoDA, 2, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": "Extract data from 2016 to 2018",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "microsoft": {},
        "collapsed": false
      },
      "source": [
        "# Load the data from storage\r\n",
        "df = spark.read.load('abfss://data@dpsyndlsdemodev.dfs.core.windows.net/sourceData/nycTripYellow2019Jan/nycTripYellow2019Jan.snappy.parquet', format='parquet')\r\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Show the column name & data type\r\n",
        "display(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Build some baseline understanding of the data.\r\n",
        "\r\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# We notice the values for fareAmount is a bit strange. \r\n",
        "# We have value <= 0 and > 1,000.\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "# Normal distribution \r\n",
        "# We want to make sure we exclude data on each extreme (99.7% coverage)\r\n",
        "\r\n",
        "dfFareAmountPercentile = df.select(percentile_approx(df[\"fareAmount\"], [0.0015, 0.9985]).alias(\"percentRank\"))\r\n",
        "\r\n",
        "display(dfFareAmountPercentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Lets plot this data to see the data as we expect a relationship between trip distance and total amount.\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from pandas.plotting import scatter_matrix\r\n",
        "\r\n",
        "# Clear the plot\r\n",
        "plt.clf()\r\n",
        "\r\n",
        "# We can add additional variables in the select for additional analysis.\r\n",
        "dfDqCheck = df.filter((col(\"fareAmount\") >= 2.5) & (col(\"fareAmount\") <= 75))\r\n",
        "pdDqCheck = dfDqCheck.select(\"tripDistance\", \"fareAmount\").toPandas()\r\n",
        "pd.plotting.scatter_matrix(pdDqCheck, figsize=(12,12))\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# As we can see, there are some outlier values for trip distance as well. \r\n",
        "# The same approach can be applied. \r\n",
        "\r\n",
        "# This is not to say, for trips that are < $2.5 and > $75 are not valid records. We should further investigate those records to ensure they are valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Lets create a table of this data\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS dataExplorationSparkDb\")\r\n",
        "dfDqCheck.write.mode(\"overwrite\").saveAsTable(\"dataExplorationSparkDb.nycTripYl2019JanClean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Clean up\r\n",
        "# spark.sql(\"DROP DATABASE IF EXISTS dataExplorationSparkDb CASCADE\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}